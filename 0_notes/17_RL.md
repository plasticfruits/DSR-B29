# Reinforcement Learning

* Aim: find optimal behavior for a problem from experience
* The Agent can start without knowing the goal
* Agent learns from either positive or negative reward (and the size of these rewards)
* Methods: 
    - Monte-Carlo
    - Q-learning (most famous)

## Libraries
* gym
* pygame

## Requirments
Interaction with environment:
1. Being able to read state from environment
2. Submit an action to environment
    - return reward
    - return new state
    - return termination
3. Reset environment

* Training dataset is generate from gathered experiences


## Terminology
* **State:** the current situation
    - based on the state the agent will take an action
* **Action space:** a collection of actions you can choose from
* **Environment:** the consequence of the action taken and the "transition" to the next (new) state
    - Here the +- reward takes place and the termination
* **Policy:** the mapping between state to actions (inside agent)
    - "whats happening in the head of your agent"
    - "an instruction guide"
    - Do this ... and then ... and then, and if ...
* **Termination:** the end of the state
* **Impossible actions:** can define as very negative reward (penalize)
* **Goal:** find a policy for a sequential decision-making problem
* **Optimal solution:** find actions that maximise the sum of all rewards
* **Reward:** is a function of state and action
* **Reward types:**
    - Immediate reward: 
    - Expected reward: sum of all rewards
    - 
* **Reward hypothesis:** "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).”
    - Reward has to be a single scalar (single-objective optimization problem)
* **non-stationary environment:** when environment changes due to learning of changes in environment (i.e. two agents playing against each other)
* 

## Policy Evaluation
* Policy `π`: a policy dictates the actions that the agent takes as a function of the agent’s state and the environment.
* Q-function: 
* **Better / improved policy:** "A policy is better than another policy if for every starting point you get more rewards."
    - if in at least one state the new policy is better it affects the entire policy.
    - How we improve a policy at a given state?
        - If you can evaluate a policy you can improve the policy.
        - 
* **Policy evaluation:** finding the values for each policy
* **Policy improvement:** evaluating policy after improvement
    - Done for each single state improvement
    - We always start with probabilistic policy (instead of deterministic)
* **Greedy strategy:**
* **Epsilon Greedy strategy:** modifying probabilities of each state to increase the better performing one
    - The smaller the epsilon the closer you get to the greedy version
    - epsilon = 1 is full exploration, 0 full exploitation
* **The explore and exploit dilemma:** you always want to explore states before adjusting epsilon to find best process, but you also want to find the process early.

## SARSA
A method for policy evaluation. 
- Uses the ***temporal difference error*** to compute gradient descent on state's. By doing this many times you converge to the q-values of policy.
- Applies improvement by taking the argmax of Q (SARSA control)


## Q-learning
"Evaluation together with improvement"




## Functions
* `env.step()` 


### Bad REwards
- cobras in india, rat tails in UK lead to opposite effect

## Examples
- Path finding: minimizing time/energy to go from A to B

##